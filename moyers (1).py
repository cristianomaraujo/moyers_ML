# -*- coding: utf-8 -*-
"""Moyers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wvjiQjYLLPNHoemANo2w6Syu_sVSD8ct
"""

import pandas as pd

# Carregar o arquivo Excel
file_path = "/content/dataset3b.xlsx"
xls = pd.ExcelFile(file_path)

# Carregar os dados
df_data = pd.read_excel(xls, sheet_name='Data')
df_moyers = pd.read_excel(xls, sheet_name='Moyers table')

# Definir parâmetros
confidence_levels = [50, 65, 75, 85, 95]
arches = ['Mandibular', 'Maxillary']

# Preencher as colunas de acordo com a tabela de Moyers
for index, row in df_data.iterrows():
    som_II_aprox = row['som_II_aprox']
    sex = row['sex']

    for conf in confidence_levels:
        for arch in arches:
            moyers_value = df_moyers[
                (df_moyers['Arch'] == arch) &
                (df_moyers['sex'] == sex) &
                (df_moyers['confidence_level'] == conf)
            ][som_II_aprox].values

            if len(moyers_value) > 0:
                df_data.loc[index, f'moyers_{arch.lower()}_{conf}%'] = moyers_value[0]

# Salvar o resultado no ambiente do Colab
output_path = "/content/updated_dataset3b.xlsx"
df_data.to_excel(output_path, index=False)

import pandas as pd
import warnings
warnings.filterwarnings("ignore")
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
import matplotlib.pyplot as plt

file_path = '/content/dataset4b.csv'
df = pd.read_csv(file_path, delimiter=';')

df.columns

df = df.dropna()

df.info()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


# Calculando os limites de outliers usando o método do IQR
def find_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]
    return outliers, lower_bound, upper_bound

outliers_ERI, lb_ERI, ub_ERI = find_outliers_iqr(df, 'ERI')
outliers_ERS, lb_ERS, ub_ERS = find_outliers_iqr(df, 'ERS')

print("Outliers na coluna ERI:")
print(outliers_ERI)

print("\nOutliers na coluna ERS:")
print(outliers_ERS)

# Remover os outliers do dataframe
df = df[(df['ERI'] >= lb_ERI) & (df['ERI'] <= ub_ERI) & (df['ERS'] >= lb_ERS) & (df['ERS'] <= ub_ERS)]

df.info()

"""**LOWER REQUIRED SPACE**"""

#colunas = ['sex', 'ERI', 'som_II', 'som_IS', 'som_molS', 'som_molI']
colunas = ['sex', 'ERI', 'som_II']
df_l = df.loc[:, colunas]

coluna_grupo = df_l.pop('ERI')
df_l.insert(0, 'ERI', coluna_grupo)

df_l

"""**UPPER REQUIRED SPACE**"""

#colunas_u = ['sex', 'ERS', 'som_II', 'som_IS', 'som_molS', 'som_molI']
colunas_u = ['sex', 'ERS', 'som_II']

df_u = df.loc[:, colunas_u]

coluna_grupo = df_u.pop('ERS')
df_u.insert(0, 'ERS', coluna_grupo)

df_u

"""# **LOWER REQUIRED SPACE MODEL**"""

X = df_l.iloc[:,1:]
y = df_l.iloc[:,0]

#RANDOM STATE
RANDOM_STATE = 42

RANDOM_STATE#Dataset split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state=RANDOM_STATE, shuffle=True)

##Data normalization
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""**GRADIENT BOOSTING REGRESSOR**"""

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo de Gradient Boosting
gb_model = GradientBoostingRegressor(random_state=RANDOM_STATE)

# Definir a grade de hiperparâmetros
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 5, 10]
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(gb_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_params_gb = grid_search.best_params_

# Treinar o melhor modelo encontrado
best_gb_model_cl = GradientBoostingRegressor(**best_params_gb)

# Ajustar o modelo nos dados de treino
best_gb_model_cl.fit(X_train, y_train)
y_pred_test_gb2 = best_gb_model_cl.predict(X_test)

# Calcular os erros no conjunto de teste
mse_gb = mean_squared_error(y_test, y_pred_test_gb2)
mae_gb = mean_absolute_error(y_test, y_pred_test_gb2)
rmse_gb = sqrt(mse_gb)
r2_gb = r2_score(y_test, y_pred_test_gb2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_gb = bootstrap_metrics_test(best_gb_model_cl, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_gb = ci95(metrics_test_gb['mse'])
rmse_ci_test_gb = ci95(metrics_test_gb['rmse'])
mae_ci_test_gb = ci95(metrics_test_gb['mae'])
r2_ci_test_gb = ci95(metrics_test_gb['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_gb['mse'])} (95% CI: {mse_ci_test_gb})")
print(f"Test Data RMSE: {np.mean(metrics_test_gb['rmse'])} (95% CI: {rmse_ci_test_gb})")
print(f"Test Data MAE: {np.mean(metrics_test_gb['mae'])} (95% CI: {mae_ci_test_gb})")
print(f"Test Data R2: {np.mean(metrics_test_gb['r2'])} (95% CI: {r2_ci_test_gb})")

# Realizando validação cruzada
kf_gb = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_gb2 = cross_val_predict(best_gb_model_cl, X_train, y_train, cv=kf_gb)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_gb = bootstrap_cv_metrics_regression(best_gb_model_cl, X_train, y_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_gb = ci95(metrics_cv_gb['mse'])
rmse_ci_cv_gb = ci95(metrics_cv_gb['rmse'])
mae_ci_cv_gb = ci95(metrics_cv_gb['mae'])
r2_ci_cv_gb = ci95(metrics_cv_gb['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_gb['mse'])} (95% CI: {mse_ci_cv_gb})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_gb['rmse'])} (95% CI: {rmse_ci_cv_gb})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_gb['mae'])} (95% CI: {mae_ci_cv_gb})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_gb['r2'])} (95% CI: {r2_ci_cv_gb})")

"""**Linear Regression**"""

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo de regressão
regression_model = LinearRegression()

# Definir a grade de hiperparâmetros
param_grid = {
    'fit_intercept': [True, False],
    'copy_X': [True, False],
    'n_jobs': [-1, 1],
    'positive': [True, False]
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(regression_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_params_linear = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)
# Treinar o melhor modelo encontrado
best_linear_model_cl = LinearRegression(**best_params_linear)

# Ajustar o modelo nos dados de treino
best_linear_model_cl.fit(X_train, y_train)
y_pred_test_linear2 = best_linear_model_cl.predict(X_test)

# Calcular os erros no conjunto de teste
mse_linear = mean_squared_error(y_test, y_pred_test_linear2)
mae_linear = mean_absolute_error(y_test, y_pred_test_linear2)
rmse_linear = sqrt(mse_linear)
r2_linear = r2_score(y_test, y_pred_test_linear2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_linear = bootstrap_metrics_test(best_linear_model_cl, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_linear = ci95(metrics_test_linear['mse'])
rmse_ci_test_linear = ci95(metrics_test_linear['rmse'])
mae_ci_test_linear = ci95(metrics_test_linear['mae'])
r2_ci_test_linear = ci95(metrics_test_linear['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_linear['mse'])} (95% CI: {mse_ci_test_linear})")
print(f"Test Data RMSE: {np.mean(metrics_test_linear['rmse'])} (95% CI: {rmse_ci_test_linear})")
print(f"Test Data MAE: {np.mean(metrics_test_linear['mae'])} (95% CI: {mae_ci_test_linear})")
print(f"Test Data R2: {np.mean(metrics_test_linear['r2'])} (95% CI: {r2_ci_test_linear})")

# Realizando validação cruzada
kf_linear = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_linear2 = cross_val_predict(best_linear_model_cl, X_train, y_train, cv=kf_linear)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_linear = bootstrap_cv_metrics_regression(best_linear_model_cl, X_train, y_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_linear = ci95(metrics_cv_linear['mse'])
rmse_ci_cv_linear = ci95(metrics_cv_linear['rmse'])
mae_ci_cv_linear = ci95(metrics_cv_linear['mae'])
r2_ci_cv_linear = ci95(metrics_cv_linear['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_linear['mse'])} (95% CI: {mse_ci_cv_linear})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_linear['rmse'])} (95% CI: {rmse_ci_cv_linear})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_linear['mae'])} (95% CI: {mae_ci_cv_linear})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_linear['r2'])} (95% CI: {r2_ci_cv_linear})")

"""**SVM**"""

from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo SVM
svm_model = SVR()

# Definir a grade de hiperparâmetros
param_grid = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf', 'poly'],
    'degree': [2, 3, 4]
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_params_svm = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)
# Treinar o melhor modelo encontrado
best_svm_model_cl = SVR(**best_params_svm)

# Ajustar o modelo nos dados de treino
best_svm_model_cl.fit(X_train, y_train)
y_pred_test_svm2 = best_svm_model_cl.predict(X_test)

# Calcular os erros no conjunto de teste
mse_svm = mean_squared_error(y_test, y_pred_test_svm2)
mae_svm = mean_absolute_error(y_test, y_pred_test_svm2)
rmse_svm = sqrt(mse_svm)
r2_svm = r2_score(y_test, y_pred_test_svm2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_svm = bootstrap_metrics_test(best_svm_model_cl, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_svm = ci95(metrics_test_svm['mse'])
rmse_ci_test_svm = ci95(metrics_test_svm['rmse'])
mae_ci_test_svm = ci95(metrics_test_svm['mae'])
r2_ci_test_svm = ci95(metrics_test_svm['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_svm['mse'])} (95% CI: {mse_ci_test_svm})")
print(f"Test Data RMSE: {np.mean(metrics_test_svm['rmse'])} (95% CI: {rmse_ci_test_svm})")
print(f"Test Data MAE: {np.mean(metrics_test_svm['mae'])} (95% CI: {mae_ci_test_svm})")
print(f"Test Data R2: {np.mean(metrics_test_svm['r2'])} (95% CI: {r2_ci_test_svm})")

# Realizando validação cruzada
kf_svm = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)
y_pred_cv_svm2 = cross_val_predict(best_svm_model_cl, X_train, y_train, cv=kf_svm)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_svm = bootstrap_cv_metrics_regression(best_svm_model_cl, X_train, y_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_svm = ci95(metrics_cv_svm['mse'])
rmse_ci_cv_svm = ci95(metrics_cv_svm['rmse'])
mae_ci_cv_svm = ci95(metrics_cv_svm['mae'])
r2_ci_cv_svm = ci95(metrics_cv_svm['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_svm['mse'])} (95% CI: {mse_ci_cv_svm})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_svm['rmse'])} (95% CI: {rmse_ci_cv_svm})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_svm['mae'])} (95% CI: {mae_ci_cv_svm})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_svm['r2'])} (95% CI: {r2_ci_cv_svm})")

"""**KNN**"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo KNN
knn_model = KNeighborsRegressor()

# Definir a grade de hiperparâmetros
param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'p': [1, 2]  # p=1 para distância de Manhattan, p=2 para distância Euclidiana
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_params_knn = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)
# Treinar o melhor modelo encontrado
best_knn_model_cl = KNeighborsRegressor(**best_params_knn)

# Ajustar o modelo nos dados de treino
best_knn_model_cl.fit(X_train, y_train)
y_pred_test_knn2 = best_knn_model_cl.predict(X_test)

# Calcular os erros no conjunto de teste
mse_knn = mean_squared_error(y_test, y_pred_test_knn2)
mae_knn = mean_absolute_error(y_test, y_pred_test_knn2)
rmse_knn = sqrt(mse_knn)
r2_knn = r2_score(y_test, y_pred_test_knn2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_knn = bootstrap_metrics_test(best_knn_model_cl, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_knn = ci95(metrics_test_knn['mse'])
rmse_ci_test_knn = ci95(metrics_test_knn['rmse'])
mae_ci_test_knn = ci95(metrics_test_knn['mae'])
r2_ci_test_knn = ci95(metrics_test_knn['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_knn['mse'])} (95% CI: {mse_ci_test_knn})")
print(f"Test Data RMSE: {np.mean(metrics_test_knn['rmse'])} (95% CI: {rmse_ci_test_knn})")
print(f"Test Data MAE: {np.mean(metrics_test_knn['mae'])} (95% CI: {mae_ci_test_knn})")
print(f"Test Data R2: {np.mean(metrics_test_knn['r2'])} (95% CI: {r2_ci_test_knn})")

# Realizando validação cruzada
kf_knn = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_knn2 = cross_val_predict(best_knn_model_cl, X_train, y_train, cv=kf_knn)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_knn = bootstrap_cv_metrics_regression(best_knn_model_cl, X_train, y_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_knn = ci95(metrics_cv_knn['mse'])
rmse_ci_cv_knn = ci95(metrics_cv_knn['rmse'])
mae_ci_cv_knn = ci95(metrics_cv_knn['mae'])
r2_ci_cv_knn = ci95(metrics_cv_knn['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_knn['mse'])} (95% CI: {mse_ci_cv_knn})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_knn['rmse'])} (95% CI: {rmse_ci_cv_knn})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_knn['mae'])} (95% CI: {mae_ci_cv_knn})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_knn['r2'])} (95% CI: {r2_ci_cv_knn})")

"""**Random Forest**"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo Random Forest
rf_model = RandomForestRegressor(random_state=RANDOM_STATE)

# Definir a grade de hiperparâmetros
param_grid = {
    'n_estimators': [50, 100, 200],  # Número de árvores na floresta
    'max_depth': [None, 10, 20],  # Profundidade máxima das árvores
    'min_samples_split': [2, 5, 10],  # Número mínimo de amostras necessárias para dividir um nó interno
    'min_samples_leaf': [1, 2, 4],  # Número mínimo de amostras necessárias para ser uma folha
    'max_features': ['auto', 'sqrt', 'log2']  # Número de features a serem consideradas para cada split
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_params_rf = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)
# Treinar o melhor modelo encontrado
best_rf_model_cl = RandomForestRegressor(**best_params_rf, random_state=42)

# Ajustar o modelo nos dados de treino
best_rf_model_cl.fit(X_train, y_train)
y_pred_test_rf2 = best_rf_model_cl.predict(X_test)

# Calcular os erros no conjunto de teste
mse_rf = mean_squared_error(y_test, y_pred_test_rf2)
mae_rf = mean_absolute_error(y_test, y_pred_test_rf2)
rmse_rf = sqrt(mse_rf)
r2_rf = r2_score(y_test, y_pred_test_rf2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_rf = bootstrap_metrics_test(best_rf_model_cl, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_rf = ci95(metrics_test_rf['mse'])
rmse_ci_test_rf = ci95(metrics_test_rf['rmse'])
mae_ci_test_rf = ci95(metrics_test_rf['mae'])
r2_ci_test_rf = ci95(metrics_test_rf['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_rf['mse'])} (95% CI: {mse_ci_test_rf})")
print(f"Test Data RMSE: {np.mean(metrics_test_rf['rmse'])} (95% CI: {rmse_ci_test_rf})")
print(f"Test Data MAE: {np.mean(metrics_test_rf['mae'])} (95% CI: {mae_ci_test_rf})")
print(f"Test Data R2: {np.mean(metrics_test_rf['r2'])} (95% CI: {r2_ci_test_rf})")

# Realizando validação cruzada
kf_rf = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_rf2 = cross_val_predict(best_rf_model_cl, X_train, y_train, cv=kf_rf)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_rf = bootstrap_cv_metrics_regression(best_rf_model_cl, X_train, y_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_rf = ci95(metrics_cv_rf['mse'])
rmse_ci_cv_rf = ci95(metrics_cv_rf['rmse'])
mae_ci_cv_rf = ci95(metrics_cv_rf['mae'])
r2_ci_cv_rf = ci95(metrics_cv_rf['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_rf['mse'])} (95% CI: {mse_ci_cv_rf})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_rf['rmse'])} (95% CI: {rmse_ci_cv_rf})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_rf['mae'])} (95% CI: {mae_ci_cv_rf})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_rf['r2'])} (95% CI: {r2_ci_cv_rf})")

"""**Adaboost Regressor**"""

from sklearn.ensemble import AdaBoostRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo AdaBoost
ada_model = AdaBoostRegressor(random_state=42)

# Definir a grade de hiperparâmetros
param_grid = {
    'n_estimators': [50, 100, 200],  # Número de estimadores base
    'learning_rate': [0.01, 0.1, 1.0],  # Taxa de aprendizado
    'loss': ['linear', 'square', 'exponential']  # Função de perda
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(ada_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_params_ada = grid_search.best_params_
print("Best Parameters:", best_params_ada)

# Treinar o melhor modelo encontrado
best_ada_model = AdaBoostRegressor(**best_params_ada, random_state=42)

# Ajustar o modelo nos dados de treino
best_ada_model.fit(X_train, y_train)
y_pred_test_ada = best_ada_model.predict(X_test)

# Calcular os erros no conjunto de teste
mse_ada = mean_squared_error(y_test, y_pred_test_ada)
mae_ada = mean_absolute_error(y_test, y_pred_test_ada)
rmse_ada = sqrt(mse_ada)
r2_ada = r2_score(y_test, y_pred_test_ada)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_ada = bootstrap_metrics_test(best_ada_model, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_ada = ci95(metrics_test_ada['mse'])
rmse_ci_test_ada = ci95(metrics_test_ada['rmse'])
mae_ci_test_ada = ci95(metrics_test_ada['mae'])
r2_ci_test_ada = ci95(metrics_test_ada['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_ada['mse'])} (95% CI: {mse_ci_test_ada})")
print(f"Test Data RMSE: {np.mean(metrics_test_ada['rmse'])} (95% CI: {rmse_ci_test_ada})")
print(f"Test Data MAE: {np.mean(metrics_test_ada['mae'])} (95% CI: {mae_ci_test_ada})")
print(f"Test Data R2: {np.mean(metrics_test_ada['r2'])} (95% CI: {r2_ci_test_ada})")

# Realizando validação cruzada
kf_ada = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_ada = cross_val_predict(best_ada_model, X_train, y_train, cv=kf_ada)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_ada = bootstrap_cv_metrics_regression(best_ada_model, X_train, y_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_ada = ci95(metrics_cv_ada['mse'])
rmse_ci_cv_ada = ci95(metrics_cv_ada['rmse'])
mae_ci_cv_ada = ci95(metrics_cv_ada['mae'])
r2_ci_cv_ada = ci95(metrics_cv_ada['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_ada['mse'])} (95% CI: {mse_ci_cv_ada})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_ada['rmse'])} (95% CI: {rmse_ci_cv_ada})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_ada['mae'])} (95% CI: {mae_ci_cv_ada})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_ada['r2'])} (95% CI: {r2_ci_cv_ada})")

"""# **Decision Tree**"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo Decision Tree
dt_model = DecisionTreeRegressor(random_state=42)

# Definir a grade de hiperparâmetros
param_grid = {
    'max_depth': [None, 10, 20, 30],  # Profundidade máxima da árvore
    'min_samples_split': [2, 5, 10],  # Número mínimo de amostras para dividir um nó
    'min_samples_leaf': [1, 2, 4],  # Número mínimo de amostras em uma folha
    'max_features': ['auto', 'sqrt', 'log2']  # Número de features a serem consideradas para cada split
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(dt_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_params_dt = grid_search.best_params_
print("Best Parameters:", best_params_dt)

# Treinar o melhor modelo encontrado
best_dt_model = DecisionTreeRegressor(**best_params_dt, random_state=42)

# Ajustar o modelo nos dados de treino
best_dt_model.fit(X_train, y_train)
y_pred_test_dt = best_dt_model.predict(X_test)

# Calcular os erros no conjunto de teste
mse_dt = mean_squared_error(y_test, y_pred_test_dt)
mae_dt = mean_absolute_error(y_test, y_pred_test_dt)
rmse_dt = sqrt(mse_dt)
r2_dt = r2_score(y_test, y_pred_test_dt)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_dt = bootstrap_metrics_test(best_dt_model, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_dt = ci95(metrics_test_dt['mse'])
rmse_ci_test_dt = ci95(metrics_test_dt['rmse'])
mae_ci_test_dt = ci95(metrics_test_dt['mae'])
r2_ci_test_dt = ci95(metrics_test_dt['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_dt['mse'])} (95% CI: {mse_ci_test_dt})")
print(f"Test Data RMSE: {np.mean(metrics_test_dt['rmse'])} (95% CI: {rmse_ci_test_dt})")
print(f"Test Data MAE: {np.mean(metrics_test_dt['mae'])} (95% CI: {mae_ci_test_dt})")
print(f"Test Data R2: {np.mean(metrics_test_dt['r2'])} (95% CI: {r2_ci_test_dt})")

# Realizando validação cruzada
kf_dt = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_dt = cross_val_predict(best_dt_model, X_train, y_train, cv=kf_dt)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_dt = bootstrap_cv_metrics_regression(best_dt_model, X_train, y_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_dt = ci95(metrics_cv_dt['mse'])
rmse_ci_cv_dt = ci95(metrics_cv_dt['rmse'])
mae_ci_cv_dt = ci95(metrics_cv_dt['mae'])
r2_ci_cv_dt = ci95(metrics_cv_dt['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_dt['mse'])} (95% CI: {mse_ci_cv_dt})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_dt['rmse'])} (95% CI: {rmse_ci_cv_dt})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_dt['mae'])} (95% CI: {mae_ci_cv_dt})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_dt['r2'])} (95% CI: {r2_ci_cv_dt})")

"""# **MLP**"""

from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo MLP
mlp_model = MLPRegressor(random_state=42, max_iter=5000)

# Definir a grade de hiperparâmetros
param_grid = {
    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],  # Diferentes arquiteturas de camadas ocultas
    'activation': ['relu', 'tanh'],  # Funções de ativação
    'solver': ['adam', 'lbfgs'],  # Otimizadores
    'alpha': [0.0001, 0.001, 0.01],  # Regularização L2
    'learning_rate': ['constant', 'adaptive']  # Taxa de aprendizado
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(mlp_model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)
best_params_mlp = grid_search.best_params_
print("Best Parameters:", best_params_mlp)

# Treinar o melhor modelo encontrado
best_mlp_model = MLPRegressor(**best_params_mlp, random_state=42, max_iter=5000)

# Ajustar o modelo nos dados de treino
best_mlp_model.fit(X_train, y_train)
y_pred_test_mlp = best_mlp_model.predict(X_test)

# Calcular os erros no conjunto de teste
mse_mlp = mean_squared_error(y_test, y_pred_test_mlp)
mae_mlp = mean_absolute_error(y_test, y_pred_test_mlp)
rmse_mlp = sqrt(mse_mlp)
r2_mlp = r2_score(y_test, y_pred_test_mlp)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_mlp = bootstrap_metrics_test(best_mlp_model, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_mlp = ci95(metrics_test_mlp['mse'])
rmse_ci_test_mlp = ci95(metrics_test_mlp['rmse'])
mae_ci_test_mlp = ci95(metrics_test_mlp['mae'])
r2_ci_test_mlp = ci95(metrics_test_mlp['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_mlp['mse'])} (95% CI: {mse_ci_test_mlp})")
print(f"Test Data RMSE: {np.mean(metrics_test_mlp['rmse'])} (95% CI: {rmse_ci_test_mlp})")
print(f"Test Data MAE: {np.mean(metrics_test_mlp['mae'])} (95% CI: {mae_ci_test_mlp})")
print(f"Test Data R2: {np.mean(metrics_test_mlp['r2'])} (95% CI: {r2_ci_test_mlp})")

# Realizando validação cruzada
kf_mlp = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_mlp = cross_val_predict(best_mlp_model, X_train, y_train, cv=kf_mlp)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_mlp = bootstrap_cv_metrics_regression(best_mlp_model, X_train, y_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_mlp = ci95(metrics_cv_mlp['mse'])
rmse_ci_cv_mlp = ci95(metrics_cv_mlp['rmse'])
mae_ci_cv_mlp = ci95(metrics_cv_mlp['mae'])
r2_ci_cv_mlp = ci95(metrics_cv_mlp['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_mlp['mse'])} (95% CI: {mse_ci_cv_mlp})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_mlp['rmse'])} (95% CI: {rmse_ci_cv_mlp})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_mlp['mae'])} (95% CI: {mae_ci_cv_mlp})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_mlp['r2'])} (95% CI: {r2_ci_cv_mlp})")

"""# **MOYERS METHOD - lower arch**"""

file_path = '/content/dataset4b.csv'
df = pd.read_csv(file_path, delimiter=';')

# Selecionar as colunas desejadas
colunas = ['moyers_mandibular_50%', 'moyers_mandibular_65%', 'moyers_mandibular_75%',
           'moyers_mandibular_85%', 'moyers_mandibular_95%', 'ERI','som_II']

# Criar o DataFrame com as colunas específicas
df_moyers = df.loc[:, colunas]

# Remover a coluna 'ERI' e armazená-la separadamente
coluna_grupo = df_moyers.pop('ERI')

# Multiplicar as colunas numéricas por 2
df_moyers = df_moyers

# Inserir a coluna 'ERI' de volta no início do DataFrame
df_moyers.insert(0, 'ERI', coluna_grupo)

# Remover a coluna 'som_II'
df_moyers = df_moyers.drop(columns=['som_II'])

# Multiplicar todas as colunas, exceto 'ERI', por 2
colunas_para_multiplicar = [col for col in df_moyers.columns if col != 'ERI']
df_moyers[colunas_para_multiplicar]

df_moyers

import numpy as np
import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.stats import t

# Lista das colunas preditoras
colunas_preditoras = ['moyers_mandibular_50%', 'moyers_mandibular_65%', 'moyers_mandibular_75%',
                      'moyers_mandibular_85%', 'moyers_mandibular_95%']

# Criar um dicionário para armazenar os resultados
resultados = {}

# Número de observações
n = len(df_moyers)
t_critico = t.ppf(0.975, df=n-1)  # Valor crítico da distribuição t para IC 95%

# Função para calcular o IC do R² via Bootstrap
def bootstrap_r2(y_true, y_pred, n_bootstrap=1000, alpha=0.05):
    r2_values = []
    np.random.seed(42)  # Para reprodutibilidade
    for _ in range(n_bootstrap):
        indices = np.random.choice(range(len(y_true)), size=len(y_true), replace=True)
        r2_values.append(r2_score(y_true[indices], y_pred[indices]))

    lower = np.percentile(r2_values, 100 * (alpha / 2))
    upper = np.percentile(r2_values, 100 * (1 - alpha / 2))

    return lower, upper

# Iterar sobre cada coluna preditora
for coluna in colunas_preditoras:
    erros = df_moyers['ERI'] - df_moyers[coluna]

    mse = mean_squared_error(df_moyers['ERI'], df_moyers[coluna])
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(df_moyers['ERI'], df_moyers[coluna])
    r2 = r2_score(df_moyers['ERI'], df_moyers[coluna])

    # IC 95% para MSE, RMSE e MAE
    mse_se = np.std(erros**2, ddof=1) / np.sqrt(n)
    mse_ic = t_critico * mse_se

    rmse_se = np.std(np.abs(erros), ddof=1) / np.sqrt(n)
    rmse_ic = t_critico * rmse_se

    mae_se = np.std(erros, ddof=1) / np.sqrt(n)
    mae_ic = t_critico * mae_se

    # IC 95% para R² via Bootstrap
    r2_lower, r2_upper = bootstrap_r2(df_moyers['ERI'].values, df_moyers[coluna].values)

    # Armazenar os resultados
    resultados[coluna] = {
        'MSE': mse, 'MSE_IC_95%': (mse - mse_ic, mse + mse_ic),
        'RMSE': rmse, 'RMSE_IC_95%': (rmse - rmse_ic, rmse + rmse_ic),
        'MAE': mae, 'MAE_IC_95%': (mae - mae_ic, mae + mae_ic),
        'R²': r2, 'R²_IC_95%': (r2_lower, r2_upper)
    }

# Converter para DataFrame para melhor visualização
df_resultados = pd.DataFrame(resultados).T

# Exibir os resultados
print(df_resultados)

# Supondo que você já tenha seu DataFrame `df_moyers` e `colunas_preditoras` definidas

# Ajustar os limites do eixo manualmente para focar nos dados principais e melhorar o layout
fig, axes = plt.subplots(1, 5, figsize=(20, 4), sharey=True)

for ax, coluna in zip(axes, colunas_preditoras):
    # Calcular as métricas
    mse = mean_squared_error(df_moyers['ERI'], df_moyers[coluna])
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(df_moyers['ERI'], df_moyers[coluna])
    r2 = r2_score(df_moyers['ERI'], df_moyers[coluna])

    # Scatter plot
    ax.scatter(df_moyers[coluna], df_moyers['ERI'], alpha=0.7, edgecolors='k', label="Predicted vs Actual")

    # Linha de identidade
    min_val, max_val = 10, 40  # Limites manuais
    ax.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label="Identity Line")

    # Ajustar limites do eixo
    ax.set_xlim(min_val, max_val)
    ax.set_ylim(min_val, max_val)

    # Título e rótulos
    ax.set_title(f"{coluna}", fontsize=10)
    ax.set_xlabel("Predicted Values", fontsize=9)
    ax.set_ylabel("ERI" if coluna == 'moyers_mandibular_50%' else "", fontsize=9)
    ax.grid(True)

    # Adicionar texto com métricas no canto inferior direito
    ax.text(46.5, 35.7, f"MSE: {mse:.2f}\nRMSE: {rmse:.2f}\nMAE: {mae:.2f}\n$R^2$: {r2:.2f}",
            fontsize=9, bbox=dict(facecolor='white', alpha=0.8))

    # Legenda menor
    ax.legend(fontsize=8, loc="upper left")

# Título geral
plt.suptitle("Scatter Plots - Moyers Method Predictions vs ERI (Adjusted Scale)", fontsize=14)
plt.tight_layout(rect=[0, 0, 1, 0.95])

# Mostrar o gráfico
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Lista de colunas preditoras de Moyers
colunas_preditoras = ['moyers_mandibular_50%', 'moyers_mandibular_65%',
                       'moyers_mandibular_75%', 'moyers_mandibular_85%',
                       'moyers_mandibular_95%']

# Definição das previsões dos modelos treinados
model_predictions = {
    "Gradient Boosting": best_gb_model_cl.predict(X_test),
    "Linear Regression": best_linear_model_cl.predict(X_test),
    "SVM": best_svm_model_cl.predict(X_test),
    "KNN": best_knn_model_cl.predict(X_test),
    "Random Forest": best_rf_model_cl.predict(X_test),
    "AdaBoost": best_ada_model.predict(X_test),
    "Decision Tree": best_dt_model.predict(X_test),
    "MLP": best_mlp_model.predict(X_test)  # Adicionado MLP
}

# Determinar número total de subplots (Moyers + Modelos)
num_plots = len(colunas_preditoras) + len(model_predictions)
num_rows = 2  # Mantemos 2 linhas, uma para Moyers e outra para os modelos
num_cols = (num_plots + 1) // 2  # Ajustamos para garantir que todos apareçam

# Criar a figura e os eixos dos subplots
fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 10), sharey=True)
axes = axes.flatten()  # Transformar os eixos em uma lista para facilitar a iteração

# Limites manuais para melhor visualização
min_val, max_val = 0, 50

# Função para calcular métricas com IC 95% via bootstrap
def calculate_metrics_with_ci(y_true, y_pred, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    mae_list, rmse_list, r2_list = [], [], []

    for _ in range(n_iterations):
        indices = np.random.choice(len(y_true), len(y_true), replace=True)
        y_true_resampled = y_true[indices]
        y_pred_resampled = y_pred[indices]

        mae_list.append(mean_absolute_error(y_true_resampled, y_pred_resampled))
        rmse_list.append(np.sqrt(mean_squared_error(y_true_resampled, y_pred_resampled)))
        r2_list.append(r2_score(y_true_resampled, y_pred_resampled))

    metrics = {
        "MAE": (np.mean(mae_list), np.percentile(mae_list, [2.5, 97.5])),
        "RMSE": (np.mean(rmse_list), np.percentile(rmse_list, [2.5, 97.5])),
        "R²": (np.mean(r2_list), np.percentile(r2_list, [2.5, 97.5]))
    }
    return metrics

# Plotando os métodos de Moyers
for i, coluna in enumerate(colunas_preditoras):
    ax = axes[i]
    metrics = calculate_metrics_with_ci(df_moyers['ERI'].values, df_moyers[coluna].values)

    mae, mae_ci = metrics["MAE"]
    rmse, rmse_ci = metrics["RMSE"]
    r2, r2_ci = metrics["R²"]

    ax.scatter(df_moyers[coluna], df_moyers['ERI'], alpha=0.7, edgecolors='k', label="Predicted vs Actual")
    ax.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label="Identity Line")

    ax.set_xlim(min_val, max_val)
    ax.set_ylim(0, 50)
    ax.set_title(f"{coluna}", fontsize=10)
    ax.set_xlabel("Predicted Values", fontsize=9)
    ax.set_ylabel("ERI" if i == 0 else "", fontsize=9)
    ax.grid(True)

    ax.text(18.4, 2.4, f"MAE: {mae:.2f} ({mae_ci[0]:.2f} - {mae_ci[1]:.2f})\n"
                     f"RMSE: {rmse:.2f} ({rmse_ci[0]:.2f} - {rmse_ci[1]:.2f})\n"
                     f"$R^2$: {r2:.2f} ({r2_ci[0]:.2f} - {r2_ci[1]:.2f})",
            fontsize=8.3, bbox=dict(facecolor='white', alpha=0.8))

    ax.legend(fontsize=8, loc="upper left")

# Plotando os modelos treinados
for i, (modelo, predicoes) in enumerate(model_predictions.items(), start=len(colunas_preditoras)):
    ax = axes[i]
    metrics = calculate_metrics_with_ci(y_test.values, predicoes)

    mae, mae_ci = metrics["MAE"]
    rmse, rmse_ci = metrics["RMSE"]
    r2, r2_ci = metrics["R²"]

    ax.scatter(predicoes, y_test, alpha=0.7, edgecolors='k', label="Predicted vs Actual")
    ax.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label="Identity Line")

    ax.set_xlim(min_val, max_val)
    ax.set_ylim(0, 50)
    ax.set_title(modelo, fontsize=10)
    ax.set_xlabel("Predicted Values", fontsize=9)
    ax.grid(True)

    ax.text(18.4, 2.4, f"MAE: {mae:.2f} ({mae_ci[0]:.2f} - {mae_ci[1]:.2f})\n"
                     f"RMSE: {rmse:.2f} ({rmse_ci[0]:.2f} - {rmse_ci[1]:.2f})\n"
                     f"$R^2$: {r2:.2f} ({r2_ci[0]:.2f} - {r2_ci[1]:.2f})",
            fontsize=8.3, bbox=dict(facecolor='white', alpha=0.8))

    ax.legend(fontsize=8, loc="upper left")

# Desativar subplots vazios
for i in range(num_plots, len(axes)):
    fig.delaxes(axes[i])

# Título geral do gráfico
plt.suptitle("Scatter Plots - Moyers Method Predictions vs ERI & Machine Learning Models", fontsize=14)
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.savefig('Scatter_RSL_with_CI95_Metrics.jpg', dpi=600, bbox_inches='tight')

# Exibir o gráfico
plt.show()

# Recarregar os dados e modelos necessários
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error
from sklearn.utils import resample

# Previsões dos modelos
y_pred_gb = best_gb_model_cl.predict(X_test)
y_pred_lr = best_linear_model_cl.predict(X_test)
y_pred_svm = best_svm_model_cl.predict(X_test)
y_pred_knn = best_knn_model_cl.predict(X_test)
y_pred_rf = best_rf_model_cl.predict(X_test)
y_pred_mlp = best_mlp_model.predict(X_test)
y_pred_dt = best_dt_model.predict(X_test)
y_pred_ada = best_ada_model.predict(X_test)

# Cálculo do MAE para cada modelo
mae_gb = mean_absolute_error(y_test, y_pred_gb)
mae_lr = mean_absolute_error(y_test, y_pred_lr)
mae_svm = mean_absolute_error(y_test, y_pred_svm)
mae_knn = mean_absolute_error(y_test, y_pred_knn)
mae_rf = mean_absolute_error(y_test, y_pred_rf)
mae_mlp = mean_absolute_error(y_test, y_pred_mlp)
mae_dt = mean_absolute_error(y_test, y_pred_dt)
mae_ada = mean_absolute_error(y_test, y_pred_ada)

# Função para calcular diferença de MAE com bootstrapping
def compare_mae_bootstrap(y_true, y_pred1, y_pred2, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    mae_diffs = []
    for _ in range(n_iterations):
        indices = np.random.choice(len(y_true), len(y_true), replace=True)
        y_true_resampled = y_true[indices]
        y_pred1_resampled = y_pred1[indices]
        y_pred2_resampled = y_pred2[indices]

        mae1 = mean_absolute_error(y_true_resampled, y_pred1_resampled)
        mae2 = mean_absolute_error(y_true_resampled, y_pred2_resampled)
        mae_diffs.append(mae1 - mae2)

    mean_diff = np.mean(mae_diffs)
    ci_diff = np.percentile(mae_diffs, [2.5, 97.5])

    return mean_diff, ci_diff

# Dicionário com previsões e nomes dos modelos
model_names = ["gb", "lr", "svm", "knn", "rf", "mlp", "dt", "ada"]
y_preds = {
    "gb": y_pred_gb,
    "lr": y_pred_lr,
    "svm": y_pred_svm,
    "knn": y_pred_knn,
    "rf": y_pred_rf,
    "mlp": y_pred_mlp,
    "dt": y_pred_dt,
    "ada": y_pred_ada
}

# Comparações entre os modelos
model_pairs = [(m1, m2) for i, m1 in enumerate(model_names) for m2 in model_names[i+1:]]
mean_diffs = []
cis = []

for model1, model2 in model_pairs:
    mean_diff, ci_diff = compare_mae_bootstrap(np.array(y_test), np.array(y_preds[model1]), np.array(y_preds[model2]))
    mean_diffs.append(mean_diff)
    cis.append(ci_diff)

# Criar os limites inferior e superior do IC95%
lower_bounds = np.array([ci[0] for ci in cis])
upper_bounds = np.array([ci[1] for ci in cis])

# Rótulos dos pares de modelos
model_labels = [f"{m1.upper()} vs {m2.upper()}" for m1, m2 in model_pairs]

# Criar gráfico de diferenças de MAE com IC95%
plt.figure(figsize=(16, 10))
bars = plt.bar(model_labels, mean_diffs, yerr=[np.abs(lower_bounds - np.array(mean_diffs)), np.abs(upper_bounds - np.array(mean_diffs))],
               capsize=8, color=['lightblue' if md >= 0 else 'lightcoral' for md in mean_diffs],
               edgecolor=['black' if md >= 0 else 'darkred' for md in mean_diffs])

plt.axhline(y=0, color='black', linestyle='--', linewidth=1.5)  # Linha horizontal em y=0
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.yticks(fontsize=12)
plt.xlabel('Model Pairs', fontsize=14)
plt.ylabel('Difference in MAE', fontsize=14)
plt.title('Difference in MAE between Model Pairs with 95% CI', fontsize=16)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Criar legenda
legend_labels = ['Positive Difference', 'Negative Difference']
legend_handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in ['lightblue', 'lightcoral']]
plt.legend(legend_handles, legend_labels, loc='upper left', fontsize=12)

# Criar texto explicativo
legend_text = {
    "GB": "Gradient Boosting Regressor",
    "LR": "Linear Regression",
    "SVM": "Support Vector Machine Regressor",
    "KNN": "K-Nearest Neighbors Regressor",
    "RF": "Random Forest Regressor",
    "MLP": "Multi-layer Perceptron Regressor",
    "DT": "Decision Tree Regressor",
    "ADA": "AdaBoost Regressor"
}

plt.text(0.74, 0.98, "\n".join(f"{key}: {value}" for key, value in legend_text.items()),
         transform=plt.gca().transAxes, fontsize=12, verticalalignment='top',
         bbox=dict(facecolor='white', alpha=0.8))

plt.tight_layout()
plt.savefig('Meandifference_reg.jpg', dpi=300, bbox_inches='tight')
plt.show()

import numpy as np
import matplotlib.pyplot as plt

X = df_l.drop('ERI', axis=1)
features = X.columns

imp_features_gb = best_gb_model_cl.feature_importances_
imp_features_lr = np.abs(best_linear_model_cl.coef_.ravel())  # Garantir que é um array 1D
imp_features_dt = best_dt_model.feature_importances_
imp_features_rf = best_rf_model_cl.feature_importances_
imp_features_ada = best_ada_model.feature_importances_

sorted_indices_gb = np.argsort(imp_features_gb)
sorted_indices_lr = np.argsort(imp_features_lr)
sorted_indices_dt = np.argsort(imp_features_dt)
sorted_indices_rf = np.argsort(imp_features_rf)
sorted_indices_ada = np.argsort(imp_features_ada)

fig, axs = plt.subplots(2, 3, figsize=(15, 10))
fig.suptitle('Feature Importance', fontsize=16)

colors = plt.cm.viridis(np.linspace(0.2, 0.8, 5))


for ax in axs.flat:
    ax.set_facecolor('#f0f0f0')  # Fundo cinza claro
    ax.grid(True, which='both', axis='both', linestyle='--', alpha=0.7)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['bottom'].set_visible(False)

# PLOT 1: Gradient Boosting
axs[0, 0].barh(features[sorted_indices_gb], imp_features_gb[sorted_indices_gb], color=colors[0])
axs[0, 0].set_title('Gradient Boosting Regressor')
axs[0, 0].set_xlabel('Importance')
axs[0, 0].set_ylabel('Feature')

# PLOT 2: Logistic Regression
axs[0, 1].barh(features[sorted_indices_lr], imp_features_lr[sorted_indices_lr], color=colors[1])
axs[0, 1].set_title('Linear Regression')
axs[0, 1].set_xlabel('Importance')
axs[0, 1].set_ylabel('Feature')

# PLOT 3: Decision Tree
axs[0, 2].barh(features[sorted_indices_dt], imp_features_dt[sorted_indices_dt], color=colors[2])
axs[0, 2].set_title('Decision Tree Regressor')
axs[0, 2].set_xlabel('Importance')
axs[0, 2].set_ylabel('Feature')

# PLOT 4: Random Forest
axs[1, 0].barh(features[sorted_indices_rf], imp_features_rf[sorted_indices_rf], color=colors[3])
axs[1, 0].set_title('Random Forest Regressor')
axs[1, 0].set_xlabel('Importance')
axs[1, 0].set_ylabel('Feature')

# PLOT 5: AdaBoost
axs[1, 1].barh(features[sorted_indices_ada], imp_features_ada[sorted_indices_ada], color=colors[4])
axs[1, 1].set_title('AdaBoost Regressor')
axs[1, 1].set_xlabel('Importance')
axs[1, 1].set_ylabel('Feature')


fig.delaxes(axs[1, 2])

plt.tight_layout(rect=[0, 0, 1, 0.96], h_pad=1.5)
plt.savefig('FEATUREIMPORTANCE2.jpg', dpi=300, bbox_inches='tight')
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error
from scipy.stats import bootstrap

# List to store MAE differences and confidence intervals
results = []

# Compute the MAE for the Moyers 75% method
mae_moyers_75 = mean_absolute_error(df_moyers['ERI'], df_moyers['moyers_mandibular_75%'])

# Compute the MAE difference and confidence interval for each model
for model_name, predictions in model_predictions.items():
    mae_model = mean_absolute_error(y_test, predictions)

    # Compute the mean MAE difference between the model and Moyers 75%
    mean_difference = mae_model - mae_moyers_75

    # Compute absolute error differences
    abs_diff_errors = np.abs(y_test - predictions) - np.abs(df_moyers['ERI'] - df_moyers['moyers_mandibular_75%'])
    abs_diff_errors = abs_diff_errors[~np.isnan(abs_diff_errors)]  # Remove NaN values

    # Compute 95% confidence interval using bootstrapping
    if len(abs_diff_errors) == 0 or np.std(abs_diff_errors) == 0:
        diff_lower, diff_upper = mean_difference, mean_difference
    else:
        boot_results = bootstrap(
            (abs_diff_errors,),
            np.mean,
            confidence_level=0.95,
            random_state=42,
            n_resamples=1000,
            method='percentile'
        )
        diff_lower = float(boot_results.confidence_interval.low)
        diff_upper = float(boot_results.confidence_interval.high)

    # Store the results in a DataFrame
    results.append([model_name, mean_difference, diff_lower, diff_upper])

# Create the final DataFrame
df_results = pd.DataFrame(results, columns=['Model', 'Diff_MAE', 'CI_Lower', 'CI_Upper'])

# Remove NaN values before plotting
df_results_clean = df_results.dropna(subset=['CI_Lower', 'CI_Upper'])

# Create the bar chart for MAE differences
plt.figure(figsize=(12, 7))

bars = plt.bar(
    df_results_clean['Model'], df_results_clean['Diff_MAE'],
    yerr=[df_results_clean['Diff_MAE'] - df_results_clean['CI_Lower'],
          df_results_clean['CI_Upper'] - df_results_clean['Diff_MAE']],
    capsize=6, color='cornflowerblue', edgecolor='black', alpha=0.9
)

# Adjust Y-axis limits
y_min = df_results_clean['CI_Lower'].min() - 0.5
y_max = df_results_clean['CI_Upper'].max() + 0.5
plt.ylim(y_min, y_max)

# Add text labels above bars
for i, row in df_results_clean.iterrows():
    plt.text(i, row['Diff_MAE'] + 0.05,
             f"Δ = {row['Diff_MAE']:.2f}\n[{row['CI_Lower']:.2f}, {row['CI_Upper']:.2f}]",
             ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))

# Add a horizontal reference line at 0
plt.axhline(y=0, color='black', linestyle='--', linewidth=1.5)

# Improve chart visualization
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.yticks(fontsize=12)
plt.xlabel('Models', fontsize=14, labelpad=10)
plt.ylabel('Difference in MAE compared to Moyers 75% (± 95% CI)', fontsize=14, labelpad=10)
plt.title('Mean MAE Difference Between Models and Moyers 75% with 95% CI', fontsize=16, weight='bold')
plt.grid(axis='y', linestyle='--', alpha=0.6)

# Display the chart
plt.tight_layout()
plt.show()

"""# **UPPER REQUIRED SPACE MODEL**: ****"""

X2 = df_u.iloc[:,1:]
y2 = df_u.iloc[:,0]

RANDOM_STATE#Dataset split
X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.30, random_state=RANDOM_STATE, shuffle=True)

##Data normalization
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X2_train = scaler.fit_transform(X2_train)
X2_test = scaler.transform(X2_test)

"""**Gradient Boosting Regressorr**"""

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo de Gradient Boosting
gb_model = GradientBoostingRegressor(random_state=RANDOM_STATE)

# Definir a grade de hiperparâmetros
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 5, 10]
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(gb_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X2_train, y2_train)
best_params_gb = grid_search.best_params_

# Treinar o melhor modelo encontrado
best_gb_model_cl2 = GradientBoostingRegressor(**best_params_gb)

# Ajustar o modelo nos dados de treino
best_gb_model_cl2.fit(X2_train, y2_train)
y_pred_test_gb2 = best_gb_model_cl2.predict(X2_test)

# Calcular os erros no conjunto de teste
mse_gb2 = mean_squared_error(y2_test, y_pred_test_gb2)
mae_gb2 = mean_absolute_error(y2_test, y_pred_test_gb2)
rmse_gb2 = sqrt(mse_gb2)
r2_gb2 = r2_score(y2_test, y_pred_test_gb2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X2_test, y2_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X2_test, y2_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_gb = bootstrap_metrics_test(best_gb_model_cl2, X2_test, y2_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_gb = ci95(metrics_test_gb['mse'])
rmse_ci_test_gb = ci95(metrics_test_gb['rmse'])
mae_ci_test_gb = ci95(metrics_test_gb['mae'])
r2_ci_test_gb = ci95(metrics_test_gb['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_gb['mse'])} (95% CI: {mse_ci_test_gb})")
print(f"Test Data RMSE: {np.mean(metrics_test_gb['rmse'])} (95% CI: {rmse_ci_test_gb})")
print(f"Test Data MAE: {np.mean(metrics_test_gb['mae'])} (95% CI: {mae_ci_test_gb})")
print(f"Test Data R2: {np.mean(metrics_test_gb['r2'])} (95% CI: {r2_ci_test_gb})")

# Realizando validação cruzada
kf_gb = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_gb2 = cross_val_predict(best_gb_model_cl2, X2_train, y2_train, cv=kf_gb)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X2, y2, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X2, y2)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_gb = bootstrap_cv_metrics_regression(best_gb_model_cl2, X2_train, y2_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_gb = ci95(metrics_cv_gb['mse'])
rmse_ci_cv_gb = ci95(metrics_cv_gb['rmse'])
mae_ci_cv_gb = ci95(metrics_cv_gb['mae'])
r2_ci_cv_gb = ci95(metrics_cv_gb['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_gb['mse'])} (95% CI: {mse_ci_cv_gb})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_gb['rmse'])} (95% CI: {rmse_ci_cv_gb})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_gb['mae'])} (95% CI: {mae_ci_cv_gb})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_gb['r2'])} (95% CI: {r2_ci_cv_gb})")

"""**Linear Regression**"""

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo de regressão
regression_model = LinearRegression()

# Definir a grade de hiperparâmetros
param_grid = {
    'fit_intercept': [True, False],
    'copy_X': [True, False],
    'n_jobs': [-1, 1],
    'positive': [True, False]
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(regression_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X2_train, y2_train)
best_params_linear = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)

# Treinar o melhor modelo encontrado
best_linear_model_cl2 = LinearRegression(**best_params_linear)

# Ajustar o modelo nos dados de treino
best_linear_model_cl2.fit(X2_train, y2_train)
y_pred_test_linear2 = best_linear_model_cl2.predict(X2_test)

# Calcular os erros no conjunto de teste
mse_linear2 = mean_squared_error(y2_test, y_pred_test_linear2)
mae_linear2 = mean_absolute_error(y2_test, y_pred_test_linear2)
rmse_linear2 = sqrt(mse_linear2)
r2_linear2 = r2_score(y2_test, y_pred_test_linear2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X2_test, y2_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X2_test, y2_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_linear = bootstrap_metrics_test(best_linear_model_cl2, X2_test, y2_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_linear = ci95(metrics_test_linear['mse'])
rmse_ci_test_linear = ci95(metrics_test_linear['rmse'])
mae_ci_test_linear = ci95(metrics_test_linear['mae'])
r2_ci_test_linear = ci95(metrics_test_linear['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_linear['mse'])} (95% CI: {mse_ci_test_linear})")
print(f"Test Data RMSE: {np.mean(metrics_test_linear['rmse'])} (95% CI: {rmse_ci_test_linear})")
print(f"Test Data MAE: {np.mean(metrics_test_linear['mae'])} (95% CI: {mae_ci_test_linear})")
print(f"Test Data R2: {np.mean(metrics_test_linear['r2'])} (95% CI: {r2_ci_test_linear})")

# Realizando validação cruzada
kf_linear = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_linear2 = cross_val_predict(best_linear_model_cl2, X2_train, y2_train, cv=kf_linear)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X2, y2, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X2, y2)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_linear = bootstrap_cv_metrics_regression(best_linear_model_cl2, X2_train, y2_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_linear = ci95(metrics_cv_linear['mse'])
rmse_ci_cv_linear = ci95(metrics_cv_linear['rmse'])
mae_ci_cv_linear = ci95(metrics_cv_linear['mae'])
r2_ci_cv_linear = ci95(metrics_cv_linear['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_linear['mse'])} (95% CI: {mse_ci_cv_linear})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_linear['rmse'])} (95% CI: {rmse_ci_cv_linear})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_linear['mae'])} (95% CI: {mae_ci_cv_linear})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_linear['r2'])} (95% CI: {r2_ci_cv_linear})")

"""**SVM**"""

from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV, train_test_split, KFold, cross_val_predict
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo SVM
svm_model = SVR()

# Definir a grade de hiperparâmetros
param_grid = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf', 'poly'],
    'degree': [2, 3, 4]
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X2_train, y2_train)
best_params_svm = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)

# Treinar o melhor modelo encontrado
best_svm_model_cl2 = SVR(**best_params_svm)

# Ajustar o modelo nos dados de treino
best_svm_model_cl2.fit(X2_train, y2_train)
y_pred_test_svm2 = best_svm_model_cl2.predict(X2_test)

# Calcular os erros no conjunto de teste
mse_svm2 = mean_squared_error(y2_test, y_pred_test_svm2)
mae_svm2 = mean_absolute_error(y2_test, y_pred_test_svm2)
rmse_svm2 = sqrt(mse_svm2)
r2_svm2 = r2_score(y2_test, y_pred_test_svm2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X2_test, y2_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X2_test, y2_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_svm = bootstrap_metrics_test(best_svm_model_cl2, X2_test, y2_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_svm = ci95(metrics_test_svm['mse'])
rmse_ci_test_svm = ci95(metrics_test_svm['rmse'])
mae_ci_test_svm = ci95(metrics_test_svm['mae'])
r2_ci_test_svm = ci95(metrics_test_svm['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_svm['mse'])} (95% CI: {mse_ci_test_svm})")
print(f"Test Data RMSE: {np.mean(metrics_test_svm['rmse'])} (95% CI: {rmse_ci_test_svm})")
print(f"Test Data MAE: {np.mean(metrics_test_svm['mae'])} (95% CI: {mae_ci_test_svm})")
print(f"Test Data R2: {np.mean(metrics_test_svm['r2'])} (95% CI: {r2_ci_test_svm})")

# Realizando validação cruzada
kf_svm = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_svm2 = cross_val_predict(best_svm_model_cl2, X2_train, y2_train, cv=kf_svm)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X2, y2, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X2, y2)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_svm = bootstrap_cv_metrics_regression(best_svm_model_cl2, X2_train, y2_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_svm = ci95(metrics_cv_svm['mse'])
rmse_ci_cv_svm = ci95(metrics_cv_svm['rmse'])
mae_ci_cv_svm = ci95(metrics_cv_svm['mae'])
r2_ci_cv_svm = ci95(metrics_cv_svm['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_svm['mse'])} (95% CI: {mse_ci_cv_svm})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_svm['rmse'])} (95% CI: {rmse_ci_cv_svm})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_svm['mae'])} (95% CI: {mae_ci_cv_svm})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_svm['r2'])} (95% CI: {r2_ci_cv_svm})")

"""**KNN**"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo KNN
knn_model = KNeighborsRegressor()

# Definir a grade de hiperparâmetros
param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'p': [1, 2]  # p=1 para distância de Manhattan, p=2 para distância Euclidiana
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X2_train, y2_train)
best_params_knn = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)

# Treinar o melhor modelo encontrado
best_knn_model_cl2 = KNeighborsRegressor(**best_params_knn)

# Ajustar o modelo nos dados de treino
best_knn_model_cl2.fit(X2_train, y2_train)
y_pred_test_knn2 = best_knn_model_cl2.predict(X2_test)

# Calcular os erros no conjunto de teste
mse_knn2 = mean_squared_error(y2_test, y_pred_test_knn2)
mae_knn2 = mean_absolute_error(y2_test, y_pred_test_knn2)
rmse_knn2 = sqrt(mse_knn2)
r2_knn2 = r2_score(y2_test, y_pred_test_knn2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X2_test, y2_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X2_test, y2_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_knn = bootstrap_metrics_test(best_knn_model_cl2, X2_test, y2_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_knn = ci95(metrics_test_knn['mse'])
rmse_ci_test_knn = ci95(metrics_test_knn['rmse'])
mae_ci_test_knn = ci95(metrics_test_knn['mae'])
r2_ci_test_knn = ci95(metrics_test_knn['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_knn['mse'])} (95% CI: {mse_ci_test_knn})")
print(f"Test Data RMSE: {np.mean(metrics_test_knn['rmse'])} (95% CI: {rmse_ci_test_knn})")
print(f"Test Data MAE: {np.mean(metrics_test_knn['mae'])} (95% CI: {mae_ci_test_knn})")
print(f"Test Data R2: {np.mean(metrics_test_knn['r2'])} (95% CI: {r2_ci_test_knn})")

# Realizando validação cruzada
kf_knn = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_knn2 = cross_val_predict(best_knn_model_cl2, X2_train, y2_train, cv=kf_knn)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X2, y2, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X2, y2)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_knn = bootstrap_cv_metrics_regression(best_knn_model_cl2, X2_train, y2_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_knn = ci95(metrics_cv_knn['mse'])
rmse_ci_cv_knn = ci95(metrics_cv_knn['rmse'])
mae_ci_cv_knn = ci95(metrics_cv_knn['mae'])
r2_ci_cv_knn = ci95(metrics_cv_knn['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_knn['mse'])} (95% CI: {mse_ci_cv_knn})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_knn['rmse'])} (95% CI: {rmse_ci_cv_knn})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_knn['mae'])} (95% CI: {mae_ci_cv_knn})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_knn['r2'])} (95% CI: {r2_ci_cv_knn})")

"""**Random Forest**"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo Random Forest
rf_model = RandomForestRegressor(random_state=RANDOM_STATE)

# Definir a grade de hiperparâmetros
param_grid = {
    'n_estimators': [50, 100, 200],  # Número de árvores na floresta
    'max_depth': [None, 10, 20],  # Profundidade máxima das árvores
    'min_samples_split': [2, 5, 10],  # Número mínimo de amostras necessárias para dividir um nó interno
    'min_samples_leaf': [1, 2, 4],  # Número mínimo de amostras necessárias para ser uma folha
    'max_features': ['auto', 'sqrt', 'log2']  # Número de features a serem consideradas para cada split
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X2_train, y2_train)
best_params_rf = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)

# Treinar o melhor modelo encontrado
best_rf_model_cl2 = RandomForestRegressor(**best_params_rf, random_state=42)

# Ajustar o modelo nos dados de treino
best_rf_model_cl2.fit(X2_train, y2_train)
y_pred_test_rf2 = best_rf_model_cl2.predict(X2_test)

# Calcular os erros no conjunto de teste
mse_rf2 = mean_squared_error(y2_test, y_pred_test_rf2)
mae_rf2 = mean_absolute_error(y2_test, y_pred_test_rf2)
rmse_rf2 = sqrt(mse_rf2)
r2_rf2 = r2_score(y2_test, y_pred_test_rf2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X2_test, y2_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X2_test, y2_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_rf = bootstrap_metrics_test(best_rf_model_cl2, X2_test, y2_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_rf = ci95(metrics_test_rf['mse'])
rmse_ci_test_rf = ci95(metrics_test_rf['rmse'])
mae_ci_test_rf = ci95(metrics_test_rf['mae'])
r2_ci_test_rf = ci95(metrics_test_rf['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_rf['mse'])} (95% CI: {mse_ci_test_rf})")
print(f"Test Data RMSE: {np.mean(metrics_test_rf['rmse'])} (95% CI: {rmse_ci_test_rf})")
print(f"Test Data MAE: {np.mean(metrics_test_rf['mae'])} (95% CI: {mae_ci_test_rf})")
print(f"Test Data R2: {np.mean(metrics_test_rf['r2'])} (95% CI: {r2_ci_test_rf})")

# Realizando validação cruzada
kf_rf = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_rf2 = cross_val_predict(best_rf_model_cl2, X2_train, y2_train, cv=kf_rf)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X2, y2, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X2, y2)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_rf = bootstrap_cv_metrics_regression(best_rf_model_cl2, X2_train, y2_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_rf = ci95(metrics_cv_rf['mse'])
rmse_ci_cv_rf = ci95(metrics_cv_rf['rmse'])
mae_ci_cv_rf = ci95(metrics_cv_rf['mae'])
r2_ci_cv_rf = ci95(metrics_cv_rf['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_rf['mse'])} (95% CI: {mse_ci_cv_rf})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_rf['rmse'])} (95% CI: {rmse_ci_cv_rf})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_rf['mae'])} (95% CI: {mae_ci_cv_rf})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_rf['r2'])} (95% CI: {r2_ci_cv_rf})")

"""**Adaboost**"""

from sklearn.ensemble import AdaBoostRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo AdaBoost
ada_model = AdaBoostRegressor(random_state=42)

# Definir a grade de hiperparâmetros
param_grid = {
    'n_estimators': [50, 100, 200],  # Número de estimadores base
    'learning_rate': [0.01, 0.1, 1.0],  # Taxa de aprendizado
    'loss': ['linear', 'square', 'exponential']  # Função de perda
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(ada_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X2_train, y2_train)
best_params_ada = grid_search.best_params_
print("Best Parameters:", best_params_ada)

# Treinar o melhor modelo encontrado
best_ada_model2 = AdaBoostRegressor(**best_params_ada, random_state=42)

# Ajustar o modelo nos dados de treino
best_ada_model2.fit(X2_train, y2_train)
y_pred_test_ada2 = best_ada_model2.predict(X2_test)

# Calcular os erros no conjunto de teste
mse_ada2 = mean_squared_error(y2_test, y_pred_test_ada2)
mae_ada2 = mean_absolute_error(y2_test, y_pred_test_ada2)
rmse_ada2 = sqrt(mse_ada2)
r2_ada2 = r2_score(y2_test, y_pred_test_ada2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X2_test, y2_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X2_test, y2_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_ada = bootstrap_metrics_test(best_ada_model2, X2_test, y2_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_ada = ci95(metrics_test_ada['mse'])
rmse_ci_test_ada = ci95(metrics_test_ada['rmse'])
mae_ci_test_ada = ci95(metrics_test_ada['mae'])
r2_ci_test_ada = ci95(metrics_test_ada['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_ada['mse'])} (95% CI: {mse_ci_test_ada})")
print(f"Test Data RMSE: {np.mean(metrics_test_ada['rmse'])} (95% CI: {rmse_ci_test_ada})")
print(f"Test Data MAE: {np.mean(metrics_test_ada['mae'])} (95% CI: {mae_ci_test_ada})")
print(f"Test Data R2: {np.mean(metrics_test_ada['r2'])} (95% CI: {r2_ci_test_ada})")

# Realizando validação cruzada
kf_ada = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_ada2 = cross_val_predict(best_ada_model2, X2_train, y2_train, cv=kf_ada)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X2, y2, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X2, y2)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_ada = bootstrap_cv_metrics_regression(best_ada_model2, X2_train, y2_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_ada = ci95(metrics_cv_ada['mse'])
rmse_ci_cv_ada = ci95(metrics_cv_ada['rmse'])
mae_ci_cv_ada = ci95(metrics_cv_ada['mae'])
r2_ci_cv_ada = ci95(metrics_cv_ada['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_ada['mse'])} (95% CI: {mse_ci_cv_ada})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_ada['rmse'])} (95% CI: {rmse_ci_cv_ada})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_ada['mae'])} (95% CI: {mae_ci_cv_ada})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_ada['r2'])} (95% CI: {r2_ci_cv_ada})")

"""**Decision Tree**"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo Decision Tree
dt_model = DecisionTreeRegressor(random_state=42)

# Definir a grade de hiperparâmetros
param_grid = {
    'max_depth': [None, 10, 20, 30],  # Profundidade máxima da árvore
    'min_samples_split': [2, 5, 10],  # Número mínimo de amostras para dividir um nó
    'min_samples_leaf': [1, 2, 4],  # Número mínimo de amostras em uma folha
    'max_features': ['auto', 'sqrt', 'log2']  # Número de features a serem consideradas para cada split
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(dt_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X2_train, y2_train)
best_params_dt = grid_search.best_params_
print("Best Parameters:", best_params_dt)

# Treinar o melhor modelo encontrado
best_dt_model2 = DecisionTreeRegressor(**best_params_dt, random_state=42)

# Ajustar o modelo nos dados de treino
best_dt_model2.fit(X2_train, y2_train)
y_pred_test_dt2 = best_dt_model2.predict(X2_test)

# Calcular os erros no conjunto de teste
mse_dt2 = mean_squared_error(y2_test, y_pred_test_dt2)
mae_dt2 = mean_absolute_error(y2_test, y_pred_test_dt2)
rmse_dt = sqrt(mse_dt2)
r2_dt2 = r2_score(y2_test, y_pred_test_dt2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X2_test, y2_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X2_test, y2_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_dt = bootstrap_metrics_test(best_dt_model2, X2_test, y2_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_dt = ci95(metrics_test_dt['mse'])
rmse_ci_test_dt = ci95(metrics_test_dt['rmse'])
mae_ci_test_dt = ci95(metrics_test_dt['mae'])
r2_ci_test_dt = ci95(metrics_test_dt['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_dt['mse'])} (95% CI: {mse_ci_test_dt})")
print(f"Test Data RMSE: {np.mean(metrics_test_dt['rmse'])} (95% CI: {rmse_ci_test_dt})")
print(f"Test Data MAE: {np.mean(metrics_test_dt['mae'])} (95% CI: {mae_ci_test_dt})")
print(f"Test Data R2: {np.mean(metrics_test_dt['r2'])} (95% CI: {r2_ci_test_dt})")

# Realizando validação cruzada
kf_dt = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_dt = cross_val_predict(best_dt_model2, X2_train, y2_train, cv=kf_dt)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X2, y2, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X2, y2)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_dt = bootstrap_cv_metrics_regression(best_dt_model2, X2_train, y2_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_dt = ci95(metrics_cv_dt['mse'])
rmse_ci_cv_dt = ci95(metrics_cv_dt['rmse'])
mae_ci_cv_dt = ci95(metrics_cv_dt['mae'])
r2_ci_cv_dt = ci95(metrics_cv_dt['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_dt['mse'])} (95% CI: {mse_ci_cv_dt})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_dt['rmse'])} (95% CI: {rmse_ci_cv_dt})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_dt['mae'])} (95% CI: {mae_ci_cv_dt})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_dt['r2'])} (95% CI: {r2_ci_cv_dt})")

"""**MLP**"""

from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo MLP
mlp_model = MLPRegressor(random_state=42, max_iter=5000)

# Definir a grade de hiperparâmetros
param_grid = {
    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],  # Diferentes arquiteturas de camadas ocultas
    'activation': ['relu', 'tanh'],  # Funções de ativação
    'solver': ['adam', 'lbfgs'],  # Otimizadores
    'alpha': [0.0001, 0.001, 0.01],  # Regularização L2
    'learning_rate': ['constant', 'adaptive']  # Taxa de aprendizado
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(mlp_model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X2_train, y2_train)
best_params_mlp = grid_search.best_params_
print("Best Parameters:", best_params_mlp)

# Treinar o melhor modelo encontrado
best_mlp_model2 = MLPRegressor(**best_params_mlp, random_state=42, max_iter=5000)

# Ajustar o modelo nos dados de treino
best_mlp_model2.fit(X2_train, y2_train)
y_pred_test_mlp2 = best_mlp_model2.predict(X2_test)

# Calcular os erros no conjunto de teste
mse_mlp2 = mean_squared_error(y2_test, y_pred_test_mlp2)
mae_mlp2 = mean_absolute_error(y2_test, y_pred_test_mlp2)
rmse_mlp2 = sqrt(mse_mlp2)
r2_mlp2 = r2_score(y2_test, y_pred_test_mlp2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X2_test, y2_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X2_test, y2_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_mlp = bootstrap_metrics_test(best_mlp_model2, X2_test, y2_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_mlp = ci95(metrics_test_mlp['mse'])
rmse_ci_test_mlp = ci95(metrics_test_mlp['rmse'])
mae_ci_test_mlp = ci95(metrics_test_mlp['mae'])
r2_ci_test_mlp = ci95(metrics_test_mlp['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_mlp['mse'])} (95% CI: {mse_ci_test_mlp})")
print(f"Test Data RMSE: {np.mean(metrics_test_mlp['rmse'])} (95% CI: {rmse_ci_test_mlp})")
print(f"Test Data MAE: {np.mean(metrics_test_mlp['mae'])} (95% CI: {mae_ci_test_mlp})")
print(f"Test Data R2: {np.mean(metrics_test_mlp['r2'])} (95% CI: {r2_ci_test_mlp})")

# Realizando validação cruzada
kf_mlp = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_mlp = cross_val_predict(best_mlp_model2, X2_train, y2_train, cv=kf_mlp)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X2, y2, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X2, y2)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_mlp = bootstrap_cv_metrics_regression(best_mlp_model2, X2_train, y2_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_mlp = ci95(metrics_cv_mlp['mse'])
rmse_ci_cv_mlp = ci95(metrics_cv_mlp['rmse'])
mae_ci_cv_mlp = ci95(metrics_cv_mlp['mae'])
r2_ci_cv_mlp = ci95(metrics_cv_mlp['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_mlp['mse'])} (95% CI: {mse_ci_cv_mlp})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_mlp['rmse'])} (95% CI: {rmse_ci_cv_mlp})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_mlp['mae'])} (95% CI: {mae_ci_cv_mlp})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_mlp['r2'])} (95% CI: {r2_ci_cv_mlp})")

"""# **MOYERS METHOD - UPPER ARCH**"""

file_path = '/content/dataset4b.csv'
df = pd.read_csv(file_path, delimiter=';')

# Selecionar as colunas desejadas
colunas = ['moyers_maxillary_50%', 'moyers_maxillary_65%', 'moyers_maxillary_75%',
           'moyers_maxillary_85%', 'moyers_maxillary_95%', 'ERS','som_II']

# Criar o DataFrame com as colunas específicas
df_moyers2 = df.loc[:, colunas]

# Remover a coluna 'ERI' e armazená-la separadamente
coluna_grupo = df_moyers2.pop('ERS')

# Inserir a coluna 'ERI' de volta no início do DataFrame
df_moyers2.insert(0, 'ERS', coluna_grupo)

df_moyers2

# Lista das colunas preditoras
colunas_preditoras = ['moyers_maxillary_50%', 'moyers_maxillary_65%', 'moyers_maxillary_75%',
                      'moyers_maxillary_85%', 'moyers_maxillary_95%']

# Criar um dicionário para armazenar os resultados
resultados = {}

# Número de observações
n = len(df_moyers2)
t_critico = t.ppf(0.975, df=n-1)  # Valor crítico da distribuição t para IC 95%

# Função para calcular o IC do R² via Bootstrap
def bootstrap_r2(y_true, y_pred, n_bootstrap=1000, alpha=0.05):
    r2_values = []
    np.random.seed(42)  # Para reprodutibilidade
    for _ in range(n_bootstrap):
        indices = np.random.choice(range(len(y_true)), size=len(y_true), replace=True)
        r2_values.append(r2_score(y_true[indices], y_pred[indices]))

    lower = np.percentile(r2_values, 100 * (alpha / 2))
    upper = np.percentile(r2_values, 100 * (1 - alpha / 2))

    return lower, upper

# Iterar sobre cada coluna preditora
for coluna in colunas_preditoras:
    erros = df_moyers2['ERS'] - df_moyers2[coluna]

    mse = mean_squared_error(df_moyers2['ERS'], df_moyers2[coluna])
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(df_moyers2['ERS'], df_moyers2[coluna])
    r2 = r2_score(df_moyers2['ERS'], df_moyers2[coluna])

    # IC 95% para MSE, RMSE e MAE
    mse_se = np.std(erros**2, ddof=1) / np.sqrt(n)
    mse_ic = t_critico * mse_se

    rmse_se = np.std(np.abs(erros), ddof=1) / np.sqrt(n)
    rmse_ic = t_critico * rmse_se

    mae_se = np.std(erros, ddof=1) / np.sqrt(n)
    mae_ic = t_critico * mae_se

    # IC 95% para R² via Bootstrap
    r2_lower, r2_upper = bootstrap_r2(df_moyers2['ERS'].values, df_moyers2[coluna].values)

    # Armazenar os resultados
    resultados[coluna] = {
        'MSE': mse, 'MSE_IC_95%': (mse - mse_ic, mse + mse_ic),
        'RMSE': rmse, 'RMSE_IC_95%': (rmse - rmse_ic, rmse + rmse_ic),
        'MAE': mae, 'MAE_IC_95%': (mae - mae_ic, mae + mae_ic),
        'R²': r2, 'R²_IC_95%': (r2_lower, r2_upper)
    }

# Converter para DataFrame para melhor visualização
df_resultados2 = pd.DataFrame(resultados).T

# Exibir os resultados
print(df_resultados2)

# Ajustar os limites do eixo manualmente para focar nos dados principais e melhorar o layout
fig, axes = plt.subplots(1, 5, figsize=(20, 4), sharey=True)

for ax, coluna in zip(axes, colunas_preditoras):
    # Calcular as métricas
    mse = mean_squared_error(df_moyers2['ERS'], df_moyers2[coluna])
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(df_moyers2['ERS'], df_moyers2[coluna])
    r2 = r2_score(df_moyers2['ERS'], df_moyers2[coluna])

    # Scatter plot
    ax.scatter(df_moyers2[coluna], df_moyers2['ERS'], alpha=0.7, edgecolors='k', label="Predicted vs Actual")

    # Linha de identidade
    min_val, max_val = 5, 50  # Limites manuais
    ax.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label="Identity Line")

    # Ajustar limites do eixo
    ax.set_xlim(min_val, max_val)
    ax.set_ylim(min_val, max_val)

    # Título e rótulos
    ax.set_title(f"{coluna}", fontsize=10)
    ax.set_xlabel("Predicted Values", fontsize=9)
    ax.set_ylabel("ERS" if coluna == 'moyers_maxillary_50%' else "", fontsize=9)
    ax.grid(True)

    # Adicionar texto com métricas no canto inferior direito
    ax.text(46.5, 35.7, f"MSE: {mse:.2f}\nRMSE: {rmse:.2f}\nMAE: {mae:.2f}\n$R^2$: {r2:.2f}",
            fontsize=9, bbox=dict(facecolor='white', alpha=0.8))

    # Legenda menor
    ax.legend(fontsize=8, loc="upper left")

# Título geral
plt.suptitle("Scatter Plots - Moyers Method Predictions vs ERS (Adjusted Scale)", fontsize=14)
plt.tight_layout(rect=[0, 0, 1, 0.95])

# Mostrar o gráfico
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Função para calcular métricas com IC 95% via bootstrap
def calculate_metrics_with_ci(y_true, y_pred, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    mae_list, rmse_list, r2_list = [], [], []

    for _ in range(n_iterations):
        indices = np.random.choice(len(y_true), len(y_true), replace=True)
        y_true_resampled = y_true[indices]
        y_pred_resampled = y_pred[indices]

        mae_list.append(mean_absolute_error(y_true_resampled, y_pred_resampled))
        rmse_list.append(np.sqrt(mean_squared_error(y_true_resampled, y_pred_resampled)))
        r2_list.append(r2_score(y_true_resampled, y_pred_resampled))

    metrics = {
        "MAE": (np.mean(mae_list), np.percentile(mae_list, [2.5, 97.5])),
        "RMSE": (np.mean(rmse_list), np.percentile(rmse_list, [2.5, 97.5])),
        "R²": (np.mean(r2_list), np.percentile(r2_list, [2.5, 97.5]))
    }
    return metrics

# Definição das previsões dos modelos treinados
model_predictions = {
    "Gradient Boosting": best_gb_model_cl2.predict(X2_test),
    "Linear Regression": best_linear_model_cl2.predict(X2_test),
    "SVM": best_svm_model_cl2.predict(X2_test),
    "KNN": best_knn_model_cl2.predict(X2_test),
    "Random Forest": best_rf_model_cl2.predict(X2_test),
    "AdaBoost": best_ada_model2.predict(X2_test),
    "Decision Tree": best_dt_model2.predict(X2_test),
    "MLP": best_mlp_model2.predict(X2_test)  # Adicionado MLP
}

# Criar lista de colunas preditoras de Moyers
colunas_preditoras = ['moyers_maxillary_50%', 'moyers_maxillary_65%',
                       'moyers_maxillary_75%', 'moyers_maxillary_85%',
                       'moyers_maxillary_95%']

# Determinar número total de subplots (Moyers + Modelos)
num_plots = len(colunas_preditoras) + len(model_predictions)
num_rows = 2  # Mantemos 2 linhas, uma para Moyers e outra para os modelos
num_cols = (num_plots + 1) // 2  # Ajustamos para garantir que todos apareçam

# Criar a figura e os eixos dos subplots
fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 10), sharey=True)

# Converter `axes` em uma lista plana para facilitar a iteração
axes = axes.flatten()

# Limites manuais para melhor visualização
min_val, max_val = 0, 60

# Plotando os métodos de Moyers
for i, coluna in enumerate(colunas_preditoras):
    ax = axes[i]
    metrics = calculate_metrics_with_ci(df_moyers2['ERS'].values, df_moyers2[coluna].values)

    mae, mae_ci = metrics["MAE"]
    rmse, rmse_ci = metrics["RMSE"]
    r2, r2_ci = metrics["R²"]

    ax.scatter(df_moyers2[coluna], df_moyers2['ERS'], alpha=0.7, edgecolors='k', label="Predicted vs Actual")
    ax.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label="Identity Line")

    ax.set_xlim(min_val, max_val)
    ax.set_ylim(0, 50)
    ax.set_title(f"{coluna}", fontsize=10)
    ax.set_xlabel("Predicted Values", fontsize=9)
    ax.set_ylabel("ERS" if i == 0 else "", fontsize=9)
    ax.grid(True)

    ax.text(21.2, 3.3, f"MAE: {mae:.2f} ({mae_ci[0]:.2f} - {mae_ci[1]:.2f})\n"
                     f"RMSE: {rmse:.2f} ({rmse_ci[0]:.2f} - {rmse_ci[1]:.2f})\n"
                     f"$R^2$: {r2:.2f} ({r2_ci[0]:.2f} - {r2_ci[1]:.2f})",
            fontsize=8.3, bbox=dict(facecolor='white', alpha=0.8))

    ax.legend(fontsize=8, loc="upper left")

# Plotando os modelos treinados
for i, (modelo, predicoes) in enumerate(model_predictions.items(), start=len(colunas_preditoras)):
    ax = axes[i]
    metrics = calculate_metrics_with_ci(y2_test.values, predicoes)

    mae, mae_ci = metrics["MAE"]
    rmse, rmse_ci = metrics["RMSE"]
    r2, r2_ci = metrics["R²"]

    ax.scatter(predicoes, y2_test, alpha=0.7, edgecolors='k', label="Predicted vs Actual")
    ax.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label="Identity Line")

    ax.set_xlim(min_val, max_val)
    ax.set_ylim(0, 50)
    ax.set_title(modelo, fontsize=10)
    ax.set_xlabel("Predicted Values", fontsize=9)
    ax.grid(True)

    ax.text(21.2, 3.3, f"MAE: {mae:.2f} ({mae_ci[0]:.2f} - {mae_ci[1]:.2f})\n"
                     f"RMSE: {rmse:.2f} ({rmse_ci[0]:.2f} - {rmse_ci[1]:.2f})\n"
                     f"$R^2$: {r2:.2f} ({r2_ci[0]:.2f} - {r2_ci[1]:.2f})",
            fontsize=8.3, bbox=dict(facecolor='white', alpha=0.8))

    ax.legend(fontsize=8.3, loc="upper left")

# Desativar subplots vazios
for i in range(num_plots, len(axes)):
    fig.delaxes(axes[i])

# Título geral do gráfico
plt.suptitle("Scatter Plots - Moyers Method Predictions vs ERS & Machine Learning Models", fontsize=14)
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.savefig('Scatter_RSU_with_CI95_Metrics.jpg', dpi=600, bbox_inches='tight')

# Exibir o gráfico
plt.show()

# Recarregar os dados e modelos necessários
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error
from sklearn.utils import resample

# Previsões dos modelos
y_pred_gb2 = best_gb_model_cl2.predict(X2_test)
y_pred_lr2 = best_linear_model_cl2.predict(X2_test)
y_pred_svm2 = best_svm_model_cl2.predict(X2_test)
y_pred_knn2 = best_knn_model_cl2.predict(X2_test)
y_pred_rf2 = best_rf_model_cl2.predict(X2_test)
y_pred_mlp2 = best_mlp_model2.predict(X2_test)
y_pred_dt2 = best_dt_model2.predict(X2_test)
y_pred_ada2 = best_ada_model2.predict(X2_test)

# Cálculo do MAE para cada modelo
mae_gb = mean_absolute_error(y2_test, y_pred_gb2)
mae_lr = mean_absolute_error(y2_test, y_pred_lr2)
mae_svm = mean_absolute_error(y2_test, y_pred_svm2)
mae_knn = mean_absolute_error(y2_test, y_pred_knn2)
mae_rf = mean_absolute_error(y2_test, y_pred_rf2)
mae_mlp = mean_absolute_error(y2_test, y_pred_mlp2)
mae_dt = mean_absolute_error(y2_test, y_pred_dt2)
mae_ada = mean_absolute_error(y2_test, y_pred_ada2)

# Função para calcular diferença de MAE com bootstrapping
def compare_mae_bootstrap(y_true, y_pred1, y_pred2, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    mae_diffs = []
    for _ in range(n_iterations):
        indices = np.random.choice(len(y_true), len(y_true), replace=True)
        y_true_resampled = y_true[indices]
        y_pred1_resampled = y_pred1[indices]
        y_pred2_resampled = y_pred2[indices]

        mae1 = mean_absolute_error(y_true_resampled, y_pred1_resampled)
        mae2 = mean_absolute_error(y_true_resampled, y_pred2_resampled)
        mae_diffs.append(mae1 - mae2)

    mean_diff = np.mean(mae_diffs)
    ci_diff = np.percentile(mae_diffs, [2.5, 97.5])

    return mean_diff, ci_diff

# Dicionário com previsões e nomes dos modelos
model_names = ["gb", "lr", "svm", "knn", "rf", "mlp", "dt", "ada"]
y_preds = {
    "gb": y_pred_gb2,
    "lr": y_pred_lr2,
    "svm": y_pred_svm2,
    "knn": y_pred_knn2,
    "rf": y_pred_rf2,
    "mlp": y_pred_mlp2,
    "dt": y_pred_dt2,
    "ada": y_pred_ada2
}

# Comparações entre os modelos
model_pairs = [(m1, m2) for i, m1 in enumerate(model_names) for m2 in model_names[i+1:]]
mean_diffs = []
cis = []

for model1, model2 in model_pairs:
    mean_diff, ci_diff = compare_mae_bootstrap(np.array(y2_test), np.array(y_preds[model1]), np.array(y_preds[model2]))
    mean_diffs.append(mean_diff)
    cis.append(ci_diff)

# Criar os limites inferior e superior do IC95%
lower_bounds = np.array([ci[0] for ci in cis])
upper_bounds = np.array([ci[1] for ci in cis])

# Rótulos dos pares de modelos
model_labels = [f"{m1.upper()} vs {m2.upper()}" for m1, m2 in model_pairs]

# Criar gráfico de diferenças de MAE com IC95%
plt.figure(figsize=(16, 10))
bars = plt.bar(model_labels, mean_diffs, yerr=[np.abs(lower_bounds - np.array(mean_diffs)), np.abs(upper_bounds - np.array(mean_diffs))],
               capsize=8, color=['lightblue' if md >= 0 else 'lightcoral' for md in mean_diffs],
               edgecolor=['black' if md >= 0 else 'darkred' for md in mean_diffs])

plt.axhline(y=0, color='black', linestyle='--', linewidth=1.5)  # Linha horizontal em y=0
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.yticks(fontsize=12)
plt.xlabel('Model Pairs', fontsize=14)
plt.ylabel('Difference in MAE', fontsize=14)
plt.title('Difference in MAE between Model Pairs with 95% CI', fontsize=16)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Criar legenda
legend_labels = ['Positive Difference', 'Negative Difference']
legend_handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in ['lightblue', 'lightcoral']]
plt.legend(legend_handles, legend_labels, loc='upper left', fontsize=12)

# Criar texto explicativo
legend_text = {
    "GB": "Gradient Boosting Regressor",
    "LR": "Linear Regression",
    "SVM": "Support Vector Machine Regressor",
    "KNN": "K-Nearest Neighbors Regressor",
    "RF": "Random Forest Regressor",
    "MLP": "Multi-layer Perceptron Regressor",
    "DT": "Decision Tree Regressor",
    "ADA": "AdaBoost Regressor"
}

plt.text(0.01, 0.22, "\n".join(f"{key}: {value}" for key, value in legend_text.items()),
         transform=plt.gca().transAxes, fontsize=12, verticalalignment='top',
         bbox=dict(facecolor='white', alpha=0.7))

plt.tight_layout()
plt.savefig('Meandifference_reg_u.jpg', dpi=300, bbox_inches='tight')
plt.show()

import numpy as np
import matplotlib.pyplot as plt

X = df_u.drop('ERS', axis=1)
features = X.columns

imp_features_gb2 = best_gb_model_cl2.feature_importances_
imp_features_lr2 = np.abs(best_linear_model_cl2.coef_.ravel())  # Garantir que é um array 1D
imp_features_dt2 = best_dt_model2.feature_importances_
imp_features_rf2 = best_rf_model_cl2.feature_importances_
imp_features_ada2 = best_ada_model2.feature_importances_

sorted_indices_gb = np.argsort(imp_features_gb2)
sorted_indices_lr = np.argsort(imp_features_lr2)
sorted_indices_dt = np.argsort(imp_features_dt2)
sorted_indices_rf = np.argsort(imp_features_rf2)
sorted_indices_ada = np.argsort(imp_features_ada2)

fig, axs = plt.subplots(2, 3, figsize=(15, 10))
fig.suptitle('Feature Importance', fontsize=16)

colors = plt.cm.viridis(np.linspace(0.2, 0.8, 5))


for ax in axs.flat:
    ax.set_facecolor('#f0f0f0')  # Fundo cinza claro
    ax.grid(True, which='both', axis='both', linestyle='--', alpha=0.7)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['bottom'].set_visible(False)

# PLOT 1: Gradient Boosting
axs[0, 0].barh(features[sorted_indices_gb], imp_features_gb2[sorted_indices_gb], color=colors[0])
axs[0, 0].set_title('Gradient Boosting Regressor')
axs[0, 0].set_xlabel('Importance')
axs[0, 0].set_ylabel('Feature')

# PLOT 2: Logistic Regression
axs[0, 1].barh(features[sorted_indices_lr], imp_features_lr2[sorted_indices_lr], color=colors[1])
axs[0, 1].set_title('Linear Regression')
axs[0, 1].set_xlabel('Importance')
axs[0, 1].set_ylabel('Feature')

# PLOT 3: Decision Tree
axs[0, 2].barh(features[sorted_indices_dt], imp_features_dt2[sorted_indices_dt], color=colors[2])
axs[0, 2].set_title('Decision Tree Regressor')
axs[0, 2].set_xlabel('Importance')
axs[0, 2].set_ylabel('Feature')

# PLOT 4: Random Forest
axs[1, 0].barh(features[sorted_indices_rf], imp_features_rf2[sorted_indices_rf], color=colors[3])
axs[1, 0].set_title('Random Forest Regressor')
axs[1, 0].set_xlabel('Importance')
axs[1, 0].set_ylabel('Feature')

# PLOT 5: AdaBoost
axs[1, 1].barh(features[sorted_indices_ada], imp_features_ada2[sorted_indices_ada], color=colors[4])
axs[1, 1].set_title('AdaBoost Regressor')
axs[1, 1].set_xlabel('Importance')
axs[1, 1].set_ylabel('Feature')


fig.delaxes(axs[1, 2])

plt.tight_layout(rect=[0, 0, 1, 0.96], h_pad=1.5)
plt.savefig('FEATUREIMPORTANCE2.jpg', dpi=300, bbox_inches='tight')
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error
from scipy.stats import bootstrap

# List to store MAE differences and confidence intervals
results = []

# Compute the MAE for the Moyers 75% method
mae_moyers_75 = mean_absolute_error(df_moyers2['ERS'], df_moyers2['moyers_maxillary_75%'])

# Compute the MAE difference and confidence interval for each model
for model_name, predictions in model_predictions.items():
    mae_model = mean_absolute_error(y_test, predictions)

    # Compute the mean MAE difference between the model and Moyers 75%
    mean_difference = mae_model - mae_moyers_75

    # Compute absolute error differences
    abs_diff_errors = np.abs(y_test - predictions) - np.abs(df_moyers2['ERS'] - df_moyers2['moyers_maxillary_75%'])
    abs_diff_errors = abs_diff_errors[~np.isnan(abs_diff_errors)]  # Remove NaN values

    # Compute 95% confidence interval using bootstrapping
    if len(abs_diff_errors) == 0 or np.std(abs_diff_errors) == 0:
        diff_lower, diff_upper = mean_difference, mean_difference
    else:
        boot_results = bootstrap(
            (abs_diff_errors,),
            np.mean,
            confidence_level=0.95,
            random_state=42,
            n_resamples=1000,
            method='percentile'
        )
        diff_lower = float(boot_results.confidence_interval.low)
        diff_upper = float(boot_results.confidence_interval.high)

    # Store the results in a DataFrame
    results.append([model_name, mean_difference, diff_lower, diff_upper])

# Create the final DataFrame
df_results = pd.DataFrame(results, columns=['Model', 'Diff_MAE', 'CI_Lower', 'CI_Upper'])

# Remove NaN values before plotting
df_results_clean = df_results.dropna(subset=['CI_Lower', 'CI_Upper'])

# Create the bar chart for MAE differences
plt.figure(figsize=(12, 7))

bars = plt.bar(
    df_results_clean['Model'], df_results_clean['Diff_MAE'],
    yerr=[df_results_clean['Diff_MAE'] - df_results_clean['CI_Lower'],
          df_results_clean['CI_Upper'] - df_results_clean['Diff_MAE']],
    capsize=6, color='cornflowerblue', edgecolor='black', alpha=0.9
)

# Adjust Y-axis limits
y_min = df_results_clean['CI_Lower'].min() - 0.5
y_max = df_results_clean['CI_Upper'].max() + 0.5
plt.ylim(y_min, y_max)

# Add text labels above bars
for i, row in df_results_clean.iterrows():
    plt.text(i, row['Diff_MAE'] + 0.05,
             f"Δ = {row['Diff_MAE']:.2f}\n[{row['CI_Lower']:.2f}, {row['CI_Upper']:.2f}]",
             ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))

# Add a horizontal reference line at 0
plt.axhline(y=0, color='black', linestyle='--', linewidth=1.5)

# Improve chart visualization
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.yticks(fontsize=12)
plt.xlabel('Models', fontsize=14, labelpad=10)
plt.ylabel('Difference in MAE compared to Moyers 75% (± 95% CI)', fontsize=14, labelpad=10)
plt.title('Mean MAE Difference Between Models and Moyers 75% with 95% CI', fontsize=16, weight='bold')
plt.grid(axis='y', linestyle='--', alpha=0.6)

# Display the chart
plt.tight_layout()
plt.show()